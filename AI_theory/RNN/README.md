# RNN
- 文本数据 -> 数值
- 数值计算 -> 模型和算法
- 简单模型的构建

文本做起来非常费劲

**2012年之前**：id标记每个词，2012年google提出词向量（不同指标，每个指标为一个数值）<br>
**2017年**：*Attention is all your need.* 注意力机制告诉我们应该把注意力放在哪，找到哪些重要哪些不重要。/ 指代<br>
**2018年**：google发布bert（用了transformer架构，给出了预训练模型） <br>
**2019年**：nlp大洗牌，各种任务在bert支撑下有了显著的提升
- [bert源码]()

NLP基本都是分类任务



## 词向量 word2vec
词向量满足什么样的需求？

初始化权重参数

gensim工具包：简单

**CBOW: continuous bag of words**<br>
用上下文预测目标词<br>
- input: 目标词周围的一些词（上下文窗口）
- output: 目标词本身
**Skip-gram**
用目标词预测上下文

**negative sampling**<br>
- 标签：正样本<br>
- 其他的：都是负样本<br>

在 Word2Vec、推荐系统等场景中，我们通常需要区分“正确的匹配（正样本）”和“大量不匹配（负样本）<br>
problem: 词表或候选集合太大，不可能对所有负样本计算概率。<br>
method:随机抽取一小部分负样本参与训练，如正样本采样1个，负样本里采样9个 -> 10分类

**hard negative**<br>
在负样本中，有些“太容易识别”，没什么学习价值。比如`(cat, banana)`一看就知道没关系。但有些负样本和正样本非常像，很难区分，比如训练句子`The cat sits on the mat`中正样本`(cat, sits)`
- hard negative: `(cat, sleep)`, `(cat, sat)` 虽然不是正确答案，但和正样本语义很接近

作用：加入hard negatives，能够强迫模型学会捕捉更细致的语义差异。

### 词向量训练过程
1. 初始化词向量矩阵
2. 放到模型中学习，通过神经网络反向传播更新权重参数矩阵W，也会更新输入数据

## 递归神经网络
### 序列模型
正向：
$p(\mathbf x)=p(x_1) \cdot p(x_2|x_1) \cdot p(x_3|x_1, x_2) \cdot ... p(x_T|x_1,...,x_{T-1}) $<br>
反向：
$p(\mathbf x)=p(x_T) \cdot p(x_{T-1}|x_2) \cdot p(x_{T-2}|x_{T-1},x_T) \cdot ... p(x_1|x_2, x_3, ...,x_T) $
- 如果是真实事件，很难根据未来的事件去推前面发生的时间，因为未来事件是根据前面事件产生的。

对条件概率建模：<br>
$p(x_t|x1,...,x_{t-1}) = p(x_t|f(x_1,...x_{t-1}))$
- 对见过的数据建模，也称为自回归模型 auto regression

#### 马尔可夫假设
- 假设当前数据只跟$\tau$个过去数据点相关 -> 只需要对$\tau$个数据建模 （MLP建模即可）
$p(x_t|x_1,...,x_{t-1}) = p(x_t|x_{t-\tau},...x_{t-1})=p(x_t|f(x_{t-\tau},...,x_{t-1}))$

#### 潜变量模型
引入潜变量$h_t$来表示过去信息$h_t = f(x_1,...,x_{t-1})$<br>
- $x_t = p(x_t|h_t)$

训练两个模型：<br>
模型1：计算$h_t$的模型<br>
模型2：计算$x_t$的模型<br>


### embedding 词嵌入
每个词转换为词向量

如何把样本之间的关系连接起来？
![RNN结构图](/AI_theory/RNN/image/architecture-rnn-ltr.png)

**RNN 存在的问题**<br>
1. 串联结构，序列太长，太慢，很难学，反向传播如果出现梯度消失，整个都废了
2. 做不了太多层
3. 输入数据不会改变，词向量不会发生更新（并不适合文本任务）

transformer -> 并行计算