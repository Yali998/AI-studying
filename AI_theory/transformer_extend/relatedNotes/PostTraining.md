# Post-training

è®©æ¨¡å‹å­¦ä¼šâ€œæ›´åƒäººâ€â€œæ›´æœ‰ç”¨â€â€œæ›´å®‰å…¨â€ã€‚
åŒ…æ‹¬ï¼š 
- Supervised Finetuningï¼ˆSFTï¼‰ 
- Preference-based methodsï¼ˆå¦‚ RLHFã€DPOã€Rejection Sampling ç­‰ï¼‰

reference:<br>
[äººå·¥æ™ºèƒ½LLMæ¨¡å‹ï¼šå¥–åŠ±æ¨¡å‹çš„è®­ç»ƒã€PPO å¼ºåŒ–å­¦ä¹ çš„è®­ç»ƒã€RLHF](https://blog.csdn.net/sinat_39620217/article/details/131776129?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522986f25fe35fa20713a63ad1fd0d50c87%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&request_id=986f25fe35fa20713a63ad1fd0d50c87&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~blog~top_positive~default-1-131776129-null-null.nonecase&utm_term=%E5%A5%96%E5%8A%B1%E6%A8%A1%E5%9E%8B&spm=1018.2226.3001.4450)
## SFT: supervised finetuning
ç”¨äººç±»ç¼–å†™çš„é«˜è´¨é‡é—®ç­”æ ·æœ¬ç›´æ¥è®­ç»ƒæ¨¡å‹ï¼Œè®©å®ƒå­¦ä¼šâ€œå¥½å›ç­”â€ã€‚

data style:
```
prompt -> response
```

training goal:<br>
minimise the loss of prediction and target: cross entropy

âš ï¸ ç¼ºç‚¹ï¼š 
- æ¨¡å‹åªå­¦åˆ°æ¨¡ä»¿è®­ç»ƒæ•°æ®ï¼› 
- æ— æ³•å­¦ä¹ â€œåå¥½â€æˆ–â€œå¥–åŠ±ä¿¡å·â€ï¼› 
- ä¸€æ—¦é‡åˆ°æ¨¡ç³Šã€æœªæ ‡æ³¨çš„æ•°æ®ï¼Œä¸çŸ¥é“å“ªç§å›ç­”æ›´å¥½ã€‚

ğŸ‘‰ æ‰€ä»¥æˆ‘ä»¬éœ€è¦ä¸‹ä¸€é˜¶æ®µï¼šåå¥½å­¦ä¹ ï¼ˆPreference Learningï¼‰ã€‚

## reject sampling
åŸºäºæ‹’ç»é‡‡æ ·çš„åå¥½è®­ç»ƒï¼šç”¨äºåœ¨å¤šä¸ªå¯èƒ½å›ç­”ä¸­é€‰æ‹©â€œæœ€å¥½â€çš„é‚£ä¸€ä¸ªã€‚
æ˜¯RLHFçš„è½»é‡åŒ–æ›¿ä»£æ–¹æ¡ˆã€‚

1. ä½¿ç”¨SFTæ¨¡å‹å¯¹åŒä¸€ä¸ªpromptç”Ÿæˆå¤šä¸ªå›ç­”ï¼š${y_1, y_2, ..., y_n}$ã€‚
2. äººç±»orè‡ªåŠ¨æ¨¡å‹è¯„ä¼°è¿™äº›å›ç­”ï¼Œé€‰å‡ºæœ€å¥½çš„ä¸€ä¸ªã€‚
3. åªç”¨è¿™äº›â€œæœ€ä½³ç­”æ¡ˆâ€å†æ¬¡SFTæ¨¡å‹ï¼Œå½¢æˆæ–°æ¨¡å‹ã€‚

## RLHF: reinforcement learning from human feedback
åŸºäºäººç±»åé¦ˆçš„å¼ºåŒ–å­¦ä¹ ã€‚
```
Pretraining â†’ SFT â†’ Reward Model â†’ RLHF (PPO)
  |             |        |            |
  |             |        |            â””â”€â”€ Fine-tune to align with reward
  |             |        â””â”€â”€ Learn human preference
  |             â””â”€â”€ Learn human-written answers
  â””â”€â”€ Learn language patterns
```
### stage1: SFT

### stage2: RM(reward model)
1. äººç±»è¯„å®¡è€…å¯¹æ¨¡å‹ç”Ÿæˆçš„å¤šä¸ªå€™é€‰ç­”æ¡ˆè¿›è¡Œæ’åº
```A > B > C > D```
2. è®­ç»ƒreward model, å­¦ä¹ è¯¥åå¥½æ’åº
3. RM:
   4. input: (prompt, answer)
   5. ouput: score

promptå’Œanswerçš„åŒ¹é…åº¦è¶Šé«˜ï¼Œåˆ™å¥–åŠ±æ¨¡å‹è¾“å‡ºçš„åˆ†æ•°ä¹Ÿè¶Šé«˜ã€‚

loss function: pairwise ranking loss

### stage3: reinforcement learning
ç”¨å¼ºåŒ–å­¦ä¹ è°ƒæ•´è¯­è¨€æ¨¡å‹ï¼Œå½“æ¨¡å‹åœ¨ç”Ÿæˆå›ç­”æ—¶ï¼š
- reward modelç»™å‡ºå¥–åŠ±
- PPOä¼˜åŒ–ä½¿æ¨¡å‹è¾“å‡ºèƒ½æœ€å¤§åŒ–å¥–åŠ± $\text{maximise}~\mathbb{E}[\mathbf{R}_\theta(\text{prompt}, \text{answer})]$
- åŠ å…¥KLæƒ©ç½šé¡¹ï¼Œé˜²æ­¢æ¨¡å‹åç¦»åŸå§‹è¯­è¨€èƒ½åŠ›å¤ªè¿œ $\mathbf{L} = - \mathbf{R}_\theta + \beta \cdot \mathbf{D}_{\text{KL}}(\pi_\theta||\pi_{\text{SFT}})$

#### PPO: proximal policy optimization
è¿‘ç«¯ç­–ç•¥ä¼˜åŒ–ï¼šå¯¹è®¾å®šçš„ç›®æ ‡å‡½æ•°é€šè¿‡éšæœºæ¢¯åº¦ä¸‹é™è¿›è¡Œä¼˜åŒ–ã€‚

#### DPO: direct preference optimization
ç›´æ¥åå¥½ä¼˜åŒ–ï¼šä¸å†æ˜¾å¼è®­ç»ƒå¥–åŠ±æ¨¡å‹æˆ–ä½¿ç”¨å¼ºåŒ–å­¦ä¹ ï¼Œè€Œæ˜¯ç›´æ¥åˆ©ç”¨äººç±»åå¥½æ•°æ®ï¼ˆpreferred / dispreferred responsesï¼‰  
å¯¹è¯­è¨€æ¨¡å‹å‚æ•° $(\pi_\theta)$ è¿›è¡Œä¼˜åŒ–ã€‚

preference pair: $(x, y^+, y^-)$

$\mathcal{L}_{\text{DPO}} = -\mathbb{E}_{(x, y^+, y^-)}[\log\sigma(\beta(\log\frac{\pi_\theta(y^+|x)}{\pi_{ref}(y^+|x)} - \log\frac{\pi_\theta(y^-|x)}{\pi_{ref}(y^-|x)})]$
- $\pi_\theta$: the model
- $\pi_{ref}$: reference model (sft model)
- $\sigma(\cdot)$: sigmoid
- $\beta$: temperature coefficient

æ¨¡å‹å‚æ•°æ›´æ–°æ–¹å‘ï¼šè®©ç”Ÿæˆ$y^+$çš„æ¯ä¸ªè¯çš„æ¦‚ç‡æ›´å¤§ï¼Œè€Œç”Ÿæˆ$y^-$çš„æ¯ä¸ªè¯çš„æ¦‚ç‡æ›´å°ã€‚
